## Methods

### Datasets

#### bioRxiv
1. Describe how bioRxiv was obtained
2. Describe metadata statistics on bioRxiv (number of preprints, number of preprints with multiple versions)

#### PubMed Central
1. Describe how PubMed central was obtained
2. Describe metadata statistics on PubMed central (number of articles, how many articles were processed

### Comparing Corpora
1. Spacy to process text via - Lemmatization, removal of stop words
2. Describe counting frequencies of each lemma
3. Describe using chi-square test
4. Describe how to calculate the likelihood and log odds ratio

### Visualizing the Preprint Landscape

#### Generate Document Representation
We used the gensim [@raw:rehurek_lrec] package (version 3.8.1) to train a word2vec model [@arxiv:1301.3781] over the entire bioRxiv repository. 
We trained this model for 20 epochs with a vector size of 300 dimensions and set the seed to be 100.
The rest of the model's parameters were set to gensim's default settings.
Following the word2vec training, we generated document representation as an average of all word vectors present in a given document.

#### Dimensionality Reduction of Document Embeddings
1. Explain how tSNE works (paragraph one)
2. Explain how PCA works  (paragraph two)
3. Discuss how words were mapped onto PC components via cosine similarity
4. ^ Explain cosine similarity

### Recommending Journals/ bioRxiv Audience Analysis
1. This title will update as analysis is completed
2. This section will describe how the above process is conducted
