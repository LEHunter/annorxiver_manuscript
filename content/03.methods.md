## Methods

### Datasets

#### BioRxiv
BioRxiv [@doi:10.1101/833400] is a repository of biological and biomedical research preprints.
We downloaded an xml snapshot of this repository on February 3, 2020 from   bioRxiv's Amazon S3 resource [@https://www.biorxiv.org/tdm] that contained the full text and image content of 98,023 preprints.
Preprints on bioRxiv are versioned, and in our snapshot 26,905 of 98,023 contained more than one version.
When preprints had multiple versions, we used only the latest one. 
Preprints in this snapshot were grouped into one of twenty-nine different categories.
Each preprint was also classified as a new result, confirmatory finding, or contradictory finding.
Some preprints in this snapshot have been withdrawn from bioRxiv.
When a preprint is withdrawn, its content is replaced with the reason for withdrawal.
Because we used the latest version, withdrawn preprints in our analysis contained only statements indicating their removal.

#### PubMed Central
PubMed Central (PMC) [@doi:10.1073/pnas.98.2.381] is a repository that contains free-to-read articles.
PMC contains two types of contributions: closed access articles from research funded by the United States National Institutes of Health (NIH) appearing after an embargo period and articles published under Gold Open Access [@doi:10.1007/s12471-017-1064-2] publishing schemes.
Paper availability within PMC is largely dependent on the journal's participation level [@url:https://www.ncbi.nlm.nih.gov/pmc/about/submission-methods/].
Individual journals have can fully participate in submitting articles to PMC, selectively participate sending only a few few of papers to PMC, only submit papers according to NIH's public access policy [@url:https://grants.nih.gov/grants/policy/nihgps/html5/section_8/8.2.2_nih_public_access_policy.htm], or not participate at all.
As of September 2019, PMC had 5,725,819 articles available [@url:https://www.ncbi.nlm.nih.gov/pmc/about/intro/].
Out of these 5 million articles, about 3 million were open access and available for text processing systems [@doi:10.1093/bioinformatics/btz070;@doi:10.1093/nar/gkz389].
We downloaded a snapshot of this open access subset on January 31, 2020.
This snapshot contains papers such as literature reviews, book reviews, editorials, case reports, research articles and more; however, we used only the research articles.

### Comparing Corpora
We used gensim [@raw:rehurek_lrec] (version 3.8.1) to preprocess the bioRxiv and PubMed Central corpora.
We removed the 337 gensim-provided stopwords.
Throughout our analysis we encountered non-word symbols (e.g., $\pm$), so we refer to words and symbols as tokens to avoid confusion.

Following the cleaning process, we calculated the frequency of every token shared between both corpora.
Because many tokens were unique to one set or the other and observed at low frequency, we used the union of the top 100 most frequent tokens from each corpus to compare them.
We generated a contingency table and calculated the odds ratio for each token.
Furthermore, we also calculated the 95% confidence interval for each odds ratio [@https://www.ncbi.nlm.nih.gov/books/NBK431098/].

### Visualizing the Preprint Landscape

#### Generate Document Representation
We used gensim [@raw:rehurek_lrec] (version 3.8.1) to train a word2vec continuous bag of words (CBOW) [@arxiv:1301.3781] model over the bioRxiv corpus. 
Our neural network architecture had 300 hidden nodes, and we trained this model for 20 epochs.
We set a fixed random seed and otherwise used gensim's default settings.
Following training, we generated a document vector for every article within bioRxiv and PubMed Central.
This document vector is calculated by taking the average of every token present within a given article, ignoring those that were absent from the word2vec model.

#### Dimensionality Reduction of Document Embeddings
We used principal component analysis (PCA) [@doi:10.1111/1467-9868.00196] to project bioRxiv document vectors into a low dimensional space.
We trained this model using scikit-learn's [@raw:scikit-learn] implementation of a randomized solver [@arxiv:0909.4061] with a random seed of 100, output of 50 principal components, and default settings for all other parameters.
For each principal component we calculated its cosine similarity with  all tokens in our word2vec model's vocabulary.
We report the top 100 positive and negative scoring tokens in the form of  word clouds, where the size of each word corresponds to the magnitude of similarity and color represents positive (blue) or negative (orange) association.

### Journal Recommendation

We sought to examine the extent to which the journal that a paper would eventually be published in could be predicted from its embeddings.
In the event that the journal could be predicted embeddings, our goal was to present the recommendations on a website as a short list.
Since our focus was on the embeddings, we used a k-nearest neighbors approach in the embedding space.

We first filtered all journals that had fewer than 100 papers in the PMC dataset.
A subset of bioRxiv preprints were directly linked to papers in our PMC corpus because they had been published as open access, and we held all of these out as a gold standard test set.
We used the remainder of the PMC corpus for training and initial evaluation via cross validation.
Given our use case, we considered a list of roughly ten potential journals to be feasible to return to the user, and we considered a prediction to be correct if the correct journal appeared within the ten closest neighbors of a query.

Certain journals are topic-specific while others are broad and some journals publish hundreds of papers per year while others publish more than ten thousand per year.
Considering both these characteristics, we designed two approaches.
One, based on proximity to the query article, enabled us to provide an example of the specific article or articles that led to the prediction but could be swayed by general topic journals that are highly prolific.
We termed this one the paper-based classifier.
For the paper-based classifier, we embedded each query article into the embedding space trained on bioRxiv preprints and selected the ten nearest papers.
Because multiple papers in close proximity to each other may come from the same journal, this method returned up to ten matches but could also return fewer.
The second approach used proximity to journal centroids, which provided more domain-specific publication venues.
We termed this one the journal-based classifier.
For the journal-based classifier, we calculated the average embedding of all papers published in the journal to provide the journal centroid.
We then embeeded each query article into the embedding space and selected the ten nearest journals by their centroids.
In each case, we evaluated performance using 10-fold cross validation.
After our method was fixed, we evaluated performance on the bioRxiv preprints with linked PMC identifiers.
