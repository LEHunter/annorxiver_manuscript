## Methods

### Datasets

#### bioRxiv
1. Describe how bioRxiv was obtained
2. Describe metadata statistics on bioRxiv (number of preprints, number of preprints with multiple versions)

#### PubMed Central
1. Describe how PubMed central was obtained
2. Describe metadata statistics on PubMed central (number of articles, how many articles were processed

### Comparing Corpora
1. Spacy to process text via - Lemmatization, removal of stop words
2. Describe counting frequencies of each lemma
3. Describe using chi-square test
4. Describe how to calculate the likelihood and log odds ratio

### Visualizing the Preprint Landscape

#### Generate Document Representation
We used the gensim [@raw:rehurek_lrec] package (version 3.8.1) to train a word2vec model, continuous bag of words (CBOW), [@arxiv:1301.3781] over the entire bioRxiv repository. 
We trained this model for 20 epochs to generate vectors of 300 dimensions.
We set the random seed to be 100 and kept the other parameters as gensim's default settings.
Following training, we generated a document vector for every article within bioRxiv and PubMed Central.
This document vector is calculated by taking the average of every token present within a given article as well as the word2vec model's vocab list.

#### Dimensionality Reduction of Document Embeddings
1. Explain how tSNE works (paragraph one)
2. Explain how PCA works  (paragraph two)
3. Discuss how words were mapped onto PC components via cosine similarity
4. ^ Explain cosine similarity

### Recommending Journals/ bioRxiv Audience Analysis
1. This title will update as analysis is completed
2. This section will describe how the above process is conducted
